{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Social Media Popularity Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IggyZhao/Social-Media-Popularity-and-Marketing-Strategies-by-Iggy/blob/master/Social_Media_Popularity_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3LTq59E8UK",
        "colab_type": "text"
      },
      "source": [
        "# Document Clustering and Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS-ob0Q8E8UM",
        "colab_type": "text"
      },
      "source": [
        "*In this project, I use unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE7Qr-ccE8UN",
        "colab_type": "text"
      },
      "source": [
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S0waFW9E8UO",
        "colab_type": "text"
      },
      "source": [
        "* [Part 1: Load Data](#Part-1:-Load-Data)\n",
        "* [Part 2: Tokenizing and Stemming](#Part-2:-Tokenizing-and-Stemming)\n",
        "* [Part 3: TF-IDF](#Part-3:-TF-IDF)\n",
        "* [Part 4: K-means clustering](#Part-4:-K-means-clustering)\n",
        "* [Part 5: Topic Modeling - Latent Dirichlet Allocation](#Part-5:-Topic-Modeling---Latent-Dirichlet-Allocation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nXGAelMjJFq",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Setup Google Drive Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eT1n7oijJ8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgK3r40-jzq_",
        "colab_type": "text"
      },
      "source": [
        "Load the data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N2bb9S6jKGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = drive.CreateFile({'id':'12DFPuZqbsvJ79XdS1ssrgZMNFCDyeP88'}) # \n",
        "file.GetContentFile('data.csv')  \n",
        "# 12DFPuZqbsvJ79XdS1ssrgZMNFCDyeP88/view?usp=sharing\n",
        "\n",
        "# https://drive.google.com/file/d/1DVmZ1wQIsAZXSsJFbkG8Jz8aJzoXpTEn/view?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc9DK62BE8UP",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AURTSQISj9ZY",
        "colab_type": "text"
      },
      "source": [
        "Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjdBV8gGE8UQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e9b77b47-bdc9-40d4-e841-0690b15d843d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import gensim   # for LDA\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIj2Z8T70FZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data into dataframe\n",
        "df = pd.read_csv('data.csv', encoding= 'unicode_escape')\n",
        "\n",
        "# https://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVIWPym-8MnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoRSRJzZA7eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove missing value\n",
        "df.dropna(subset=['text'],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F92fEhKfhOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "532cf43a-014a-4280-df63-fb10b3db6c4f"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>has_hashtag</th>\n",
              "      <th>display_text_width</th>\n",
              "      <th>is_popular</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7391.000000</td>\n",
              "      <td>7391.000000</td>\n",
              "      <td>7391.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.193208</td>\n",
              "      <td>97.668786</td>\n",
              "      <td>0.151536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.394841</td>\n",
              "      <td>50.305006</td>\n",
              "      <td>0.358594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>288.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       has_hashtag  display_text_width   is_popular\n",
              "count  7391.000000         7391.000000  7391.000000\n",
              "mean      0.193208           97.668786     0.151536\n",
              "std       0.394841           50.305006     0.358594\n",
              "min       0.000000            4.000000     0.000000\n",
              "25%       0.000000           63.000000     0.000000\n",
              "50%       0.000000           89.000000     0.000000\n",
              "75%       0.000000          126.000000     0.000000\n",
              "max       1.000000          288.000000     1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbKDogiW79a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the first 1000 data as our sample data\n",
        "# data = df.loc[:1000, 'review_body'].tolist()\n",
        "data = df.loc[:, 'text'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ4KGnVeE8UX",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Tokenizing and Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "GHHIaFATE8UY",
        "colab_type": "text"
      },
      "source": [
        "Load stopwords and stemmer function from NLTK library.\n",
        "Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning.\n",
        "Stemming is the process of breaking a word down into its root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gSwiUBRE8UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "107ee863-43a2-47ed-e333-aaf818159551"
      },
      "source": [
        "# Use nltk's English stopwords.\n",
        "stopwords = nltk.corpus.stopwords.words('english') # defualt stopwords in English\n",
        "stopwords.append(\"â€™\")\n",
        "stopwords.append(\"'m\")\n",
        "stopwords.append(\"n't\")\n",
        "stopwords.append(\"br\")\n",
        "# Also added some words into the stopwords\n",
        "\n",
        "print (\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")\n",
        "print (stopwords[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We use 183 stop-words from nltk library.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtDXMCeME8Uh",
        "colab_type": "text"
      },
      "source": [
        "Use our defined functions to analyze (i.e. tokenize, stem) tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e50130X8E8Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "# REGULAR EXPRESSION\n",
        "import re\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# tokenization and stemming\n",
        "def tokenization_and_stemming(text):\n",
        "    tokens = []\n",
        "    # exclude stop words and tokenize the document, generate a list of string \n",
        "    for word in nltk.word_tokenize(text):\n",
        "        if word.lower() not in stopwords:\n",
        "            tokens.append(word.lower())\n",
        "\n",
        "    filtered_tokens = []\n",
        "    \n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):  # re 是正则表达式, 和 regularization 无瓜, 这个帮我们把一些string filter出来, 只提取文字信息, 不要emoji 和数字啥的, 包含就不要, 只留纯文字\n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    # stemming\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywzEhcMIN5K6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "21669ef7-e812-4942-9ae7-f5b73554b5fe"
      },
      "source": [
        "tokenization_and_stemming(data[0])  # data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['turn',\n",
              " 'inanim',\n",
              " 'object',\n",
              " 'think',\n",
              " \"'d\",\n",
              " 'roll',\n",
              " 'duct',\n",
              " 'tape',\n",
              " 'fix',\n",
              " 'stuff',\n",
              " 'great',\n",
              " 'way',\n",
              " 'realli',\n",
              " 'trust',\n",
              " 'anyth']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwpLkHyN8cof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "82228352-4aa9-414f-e281-ffe604a5166b"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# REGULAR EXPRESSION\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# tokenization and stemming\n",
        "def tokenization_and_lemmatizer(text):\n",
        "    tokens = []\n",
        "    # exclude stop words and tokenize the document, generate a list of string \n",
        "    for word in nltk.word_tokenize(text):\n",
        "        if word.lower() not in stopwords:\n",
        "            tokens.append(word.lower())\n",
        "\n",
        "    filtered_tokens = []\n",
        "    \n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):  \n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    # stemming\n",
        "    lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
        "    return lemmatized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1jv6pBq8cFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "58df16af-94cf-4b4b-d655-b62425c70d3d"
      },
      "source": [
        "tokenization_and_lemmatizer(data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['turned',\n",
              " 'inanimate',\n",
              " 'object',\n",
              " 'think',\n",
              " \"'d\",\n",
              " 'roll',\n",
              " 'duct',\n",
              " 'tape',\n",
              " 'fix',\n",
              " 'stuff',\n",
              " 'great',\n",
              " 'way',\n",
              " 'really',\n",
              " 'trust',\n",
              " 'anything']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwODXOpg9nXD",
        "colab_type": "text"
      },
      "source": [
        "Lemmatization looks better than Stemming. The difference between they two can be referred from:\n",
        "#### https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "QAWdFqL5E8Uo",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: TF-IDF\n",
        "\n",
        "TF: Term Frequency\n",
        "\n",
        "IDF: Inverse Document Frequency\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-XH7R4pE8Up",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "597c5dce-c679-4e88-d6dc-4732867c969f"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# define vectorizer parameters\n",
        "# TfidfVectorizer will help us to create tf-idf matrix\n",
        "# max_df : maximum document frequency for the given word\n",
        "# min_df : minimum document frequency for the given word\n",
        "# max_features: maximum number of words\n",
        "# use_idf: if not true, we only calculate tf\n",
        "# stop_words: built-in stop words\n",
        "# tokenizer: how to tokenize the document\n",
        "# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram\n",
        "tfidf_model = TfidfVectorizer(max_df=0.99, max_features=250,    # TfidfVectorizer 这个玩意: max_df 最大的 document frequency, max_features=1000最后最多只要1000 个词\n",
        "                                 min_df=0.01, stop_words='english',  # min_df=0.01 出现太少也没用, 删了吧\n",
        "                                 use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,1))  # use_idf=True 如果是 F 的话只有 TF 不乘以 IDF 了, ngram_range (1,3) 就是 123 生成三种, 1-3 的意思\n",
        "\n",
        "tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses    # fit_transform 把 fit 和 transform 结合到一次 fit 根据信息生成 dictionary, transform 把 dictionary 转化成数字\n",
        "\n",
        "print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
        "      \" reviews and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "In total, there are 7391 reviews and 94 terms.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoRH6IDVE8Ur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4c398fb4-9181-418c-a658-92f5e9c6dfee"
      },
      "source": [
        "# check the parameters\n",
        "tfidf_model.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'analyzer': 'word',\n",
              " 'binary': False,\n",
              " 'decode_error': 'strict',\n",
              " 'dtype': numpy.float64,\n",
              " 'encoding': 'utf-8',\n",
              " 'input': 'content',\n",
              " 'lowercase': True,\n",
              " 'max_df': 0.99,\n",
              " 'max_features': 250,\n",
              " 'min_df': 0.01,\n",
              " 'ngram_range': (1, 1),\n",
              " 'norm': 'l2',\n",
              " 'preprocessor': None,\n",
              " 'smooth_idf': True,\n",
              " 'stop_words': 'english',\n",
              " 'strip_accents': None,\n",
              " 'sublinear_tf': False,\n",
              " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'tokenizer': <function __main__.tokenization_and_stemming>,\n",
              " 'use_idf': True,\n",
              " 'vocabulary': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whu1pCOiE8Uv",
        "colab_type": "text"
      },
      "source": [
        "Save the terms identified by TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLdKk6n-E8Uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# words\n",
        "tf_selected_words = tfidf_model.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWvzNWQFB26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41485aae-e06d-47f8-a272-6af1aec4490c"
      },
      "source": [
        "# print out words \n",
        "tf_selected_words   # 看看选出来的词儿吧  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'d\",\n",
              " \"'s\",\n",
              " 'account',\n",
              " 'alway',\n",
              " 'amp',\n",
              " 'away',\n",
              " 'best',\n",
              " 'better',\n",
              " 'big',\n",
              " 'ca',\n",
              " 'check',\n",
              " 'chicken',\n",
              " 'click',\n",
              " 'come',\n",
              " 'contact',\n",
              " 'countri',\n",
              " 'day',\n",
              " 'dm',\n",
              " 'dog',\n",
              " 'e-mail',\n",
              " 'enter',\n",
              " 'episod',\n",
              " 'everi',\n",
              " 'favorit',\n",
              " 'feel',\n",
              " 'film',\n",
              " 'friend',\n",
              " 'good',\n",
              " 'got',\n",
              " 'great',\n",
              " 'guy',\n",
              " 'happen',\n",
              " 'happi',\n",
              " 'hear',\n",
              " 'help',\n",
              " 'hope',\n",
              " 'http',\n",
              " 'https',\n",
              " 'inform',\n",
              " 'it\\x89ûª',\n",
              " 'kfc',\n",
              " 'know',\n",
              " 'let',\n",
              " 'life',\n",
              " 'like',\n",
              " 'littl',\n",
              " 'live',\n",
              " 'll',\n",
              " 'locat',\n",
              " 'look',\n",
              " 'love',\n",
              " 'lovethemad',\n",
              " 'make',\n",
              " 'moosejaw',\n",
              " 'moosejaw.com',\n",
              " 'moosejawmad',\n",
              " 'movi',\n",
              " 'na',\n",
              " 'need',\n",
              " 'netflix',\n",
              " 'new',\n",
              " 'order',\n",
              " 'peopl',\n",
              " 'perfect',\n",
              " 'pleas',\n",
              " 'reach',\n",
              " 'realli',\n",
              " 'right',\n",
              " 'say',\n",
              " 'season',\n",
              " 'seen',\n",
              " 'someth',\n",
              " 'sorri',\n",
              " 'start',\n",
              " 'store',\n",
              " 'stori',\n",
              " 'sure',\n",
              " 'tell',\n",
              " 'thank',\n",
              " 'thing',\n",
              " 'think',\n",
              " 'time',\n",
              " 'today',\n",
              " 'tri',\n",
              " 'use',\n",
              " 've',\n",
              " 'want',\n",
              " 'watch',\n",
              " 'way',\n",
              " 'whattowatchonnetflix',\n",
              " 'win',\n",
              " 'work',\n",
              " 'world',\n",
              " 'year']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEcwtws5E8U8",
        "colab_type": "text"
      },
      "source": [
        "# Part 4: K-means clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7LJQ5i3IE8U9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# k-means clustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 3\n",
        "\n",
        "# number of clusters\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "\n",
        "clusters = km.labels_.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fboVpRAfE8U-",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. Analyze K-means Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGs4aIIME8U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create DataFrame films from all of the input files.\n",
        "product = { 'text': df[:].text, 'cluster': clusters}\n",
        "frame = pd.DataFrame(product, columns = ['text', 'cluster'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APmEUmm6E8VC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "f84bccef-7fd9-4f63-85b7-4e259a48ecc8"
      },
      "source": [
        "frame.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>If I were turned into an inanimate object I th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4 years of training? No thanks. I'm gonna star...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Eye of the Tiger playing as I push a shopping ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Back to the Future Part II was ahead of its ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Three days until Valentine's Day which means t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Someone needs to make a heated blanket that sm...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Playing a new game I call \"guess how much chan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I'm just glad we've all agreed to let somethin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@C_A_resist That's what we like to hear. We'll...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Asked the guy across from my what I should twe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  cluster\n",
              "0  If I were turned into an inanimate object I th...        1\n",
              "1  4 years of training? No thanks. I'm gonna star...        1\n",
              "2  Eye of the Tiger playing as I push a shopping ...        1\n",
              "3  Back to the Future Part II was ahead of its ti...        1\n",
              "4  Three days until Valentine's Day which means t...        1\n",
              "5  Someone needs to make a heated blanket that sm...        1\n",
              "6  Playing a new game I call \"guess how much chan...        1\n",
              "7  I'm just glad we've all agreed to let somethin...        1\n",
              "8  @C_A_resist That's what we like to hear. We'll...        1\n",
              "9  Asked the guy across from my what I should twe...        1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht1SbbOSE8VE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "31bd4294-1b94-4231-a4b0-11c39bd3a77d"
      },
      "source": [
        "print (\"Number of texts included in each cluster:\") \n",
        "frame['cluster'].value_counts().to_frame() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of texts included in each cluster:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1229</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cluster\n",
              "1     4893\n",
              "2     1269\n",
              "0     1229"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phDfUdh5qL9G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "689f7dc1-0573-4ccf-e31a-870f1745b469"
      },
      "source": [
        "km.cluster_centers_  # 把 kmeans 的 cluster 的中心点打印了出来\n",
        "\n",
        "# 此处我认为中心点也构成了一句话, 代表了我这个 cluster, 然后这个里面的值越大, 代表这个词的重要性越强, 也就是说我们可以提取出这个 cluster 里面最重要的词儿 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.60393978e-03, 5.95083445e-02, 0.00000000e+00, 4.28434701e-03,\n",
              "        1.94379299e-02, 3.34135909e-03, 2.77079203e-02, 3.05778557e-03,\n",
              "        5.73380302e-03, 4.03888885e-03, 1.34812271e-02, 9.82976626e-03,\n",
              "        1.33138740e-02, 2.80057259e-02, 6.12479359e-04, 0.00000000e+00,\n",
              "        2.19055051e-02, 3.83569614e-04, 3.50652605e-03, 0.00000000e+00,\n",
              "        8.59568378e-03, 1.68138870e-02, 7.97668393e-03, 7.64486378e-03,\n",
              "        5.36606985e-03, 1.39341792e-02, 1.11785587e-02, 1.27627478e-02,\n",
              "        1.01755241e-02, 3.84777812e-03, 6.04474830e-03, 6.71065763e-03,\n",
              "        1.21079313e-02, 2.86379390e-03, 7.48757004e-03, 2.80970967e-03,\n",
              "        2.38945896e-03, 5.50655244e-01, 9.46667002e-04, 6.02302550e-03,\n",
              "        1.50880235e-02, 1.26779391e-02, 5.96307279e-03, 1.19695800e-02,\n",
              "        1.91318832e-02, 3.72334223e-03, 6.04084407e-03, 3.20863205e-03,\n",
              "        1.14668440e-03, 1.10341579e-02, 3.31280226e-02, 1.49696709e-02,\n",
              "        1.32738094e-02, 9.91447662e-03, 0.00000000e+00, 1.48255964e-02,\n",
              "        1.68334848e-02, 8.91246948e-03, 1.27324899e-02, 7.38770615e-02,\n",
              "        4.30705089e-02, 4.62080785e-03, 1.18412649e-02, 5.26250546e-03,\n",
              "        3.21020371e-03, 0.00000000e+00, 6.45502366e-03, 7.98009971e-03,\n",
              "        1.03244955e-02, 2.44471458e-02, 3.17688784e-03, 2.41438147e-03,\n",
              "        3.14716372e-03, 9.47157526e-03, 1.60850844e-03, 1.40531190e-02,\n",
              "        3.04557432e-03, 6.38463710e-03, 9.94006797e-03, 1.02180498e-02,\n",
              "        1.23221385e-02, 1.82152947e-02, 1.55616986e-02, 7.12389214e-03,\n",
              "        6.15121268e-03, 5.37382780e-03, 1.23404548e-02, 3.70977449e-02,\n",
              "        7.72259263e-03, 0.00000000e+00, 1.61878914e-02, 5.23465471e-03,\n",
              "        8.67531296e-03, 1.46469758e-02],\n",
              "       [5.62018748e-03, 1.44401814e-02, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.57067056e-03, 0.00000000e+00, 1.79166199e-03, 1.02656268e-01,\n",
              "        1.93276851e-03, 0.00000000e+00, 5.24124896e-03, 0.00000000e+00,\n",
              "        0.00000000e+00, 9.30229607e-04, 0.00000000e+00, 0.00000000e+00,\n",
              "        9.48758416e-04, 2.16005760e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.36361168e-03, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.33791040e-03,\n",
              "        4.29321228e-03, 1.76111297e-03, 0.00000000e+00, 9.56098879e-02,\n",
              "        0.00000000e+00, 6.08877936e-03, 0.00000000e+00, 1.61235226e-03,\n",
              "        0.00000000e+00, 8.60589249e-03, 3.17233162e-01, 0.00000000e+00,\n",
              "        9.66705551e-04, 3.52493161e-03, 1.09644442e-03, 1.35049272e-03,\n",
              "        4.76823135e-01, 1.46291641e-03, 0.00000000e+00, 1.75123532e-03,\n",
              "        1.27963944e-01, 2.85853621e-01, 1.44470025e-03, 0.00000000e+00,\n",
              "        1.49374854e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 1.77578862e-03, 1.70320967e-03,\n",
              "        1.84800981e-03, 1.38848601e-03, 2.01638427e-03, 7.20824585e-04,\n",
              "        5.71662876e-02, 0.00000000e+00, 0.00000000e+00, 3.10229288e-03,\n",
              "        1.50362932e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.70899848e-02, 1.06273326e-03, 7.93796534e-02, 0.00000000e+00,\n",
              "        0.00000000e+00, 3.21217312e-03, 0.00000000e+00, 6.14032625e-03,\n",
              "        2.67752381e-03, 2.28942022e-03, 0.00000000e+00, 2.63172483e-03,\n",
              "        1.14777352e-03, 1.06833036e-03, 1.76519843e-03, 1.31972035e-03,\n",
              "        2.51543857e-03, 1.77355078e-02, 1.31248532e-03, 0.00000000e+00,\n",
              "        0.00000000e+00, 4.36062648e-03],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 3.97369512e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 3.45461788e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 1.59885938e-03, 2.70292164e-02, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.11130945e-04,\n",
              "        0.00000000e+00, 3.79272144e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        2.43591997e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 5.99593934e-04, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.63053244e-01, 0.00000000e+00, 0.00000000e+00, 2.44511831e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.68483628e-01, 0.00000000e+00, 2.63763672e-02, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        6.53326537e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 6.45796769e-01, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 3.22918791e-01, 3.23391809e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 4.89580268e-04, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.11805091e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        2.10741863e-01, 3.23556890e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        2.14829324e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.84853722e-01,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00],\n",
              "       [1.24079394e-02, 8.36535996e-02, 9.16289800e-04, 9.94559753e-03,\n",
              "        9.71529180e-03, 9.70448982e-03, 1.19270130e-02, 8.69445674e-03,\n",
              "        9.85835991e-03, 1.20461070e-02, 2.47719741e-02, 1.03691416e-02,\n",
              "        5.26652391e-03, 1.22777256e-02, 5.20603675e-04, 5.65114978e-04,\n",
              "        2.75211397e-02, 6.14224772e-03, 1.52724176e-02, 1.43430356e-02,\n",
              "        9.69076891e-03, 7.30372853e-03, 9.33935972e-03, 9.77715195e-03,\n",
              "        1.40942380e-02, 7.22140333e-03, 9.24849949e-03, 2.84666542e-02,\n",
              "        2.21777323e-02, 2.31050239e-02, 1.38778449e-02, 1.22917024e-02,\n",
              "        1.57162000e-02, 2.97334518e-03, 9.80294205e-03, 1.06855506e-02,\n",
              "        2.97299173e-02, 1.67251044e-02, 9.67501446e-04, 8.10824984e-03,\n",
              "        5.60409991e-03, 2.12163846e-02, 1.21332161e-02, 9.30382220e-03,\n",
              "        2.65169828e-02, 1.14287970e-02, 1.03359668e-02, 2.48096430e-02,\n",
              "        7.38175374e-04, 9.75670757e-03, 2.56213083e-02, 6.29875202e-03,\n",
              "        1.49643842e-02, 9.68568032e-03, 1.53462929e-02, 9.79928304e-03,\n",
              "        1.23135118e-02, 1.05117206e-02, 1.69297343e-02, 1.49809256e-02,\n",
              "        2.57084466e-02, 1.04987348e-02, 1.38540541e-02, 1.20617476e-02,\n",
              "        8.16663112e-03, 6.46423931e-04, 1.24793944e-02, 1.46229228e-02,\n",
              "        1.53382072e-02, 9.45059214e-03, 1.24618744e-02, 1.31414238e-02,\n",
              "        6.65180871e-03, 1.21075581e-02, 1.95261890e-03, 8.45313144e-03,\n",
              "        1.17651036e-02, 1.60133313e-02, 1.49732424e-02, 1.96905093e-02,\n",
              "        2.02910149e-02, 2.72707811e-02, 2.01969470e-02, 3.28021966e-02,\n",
              "        1.00330476e-02, 1.99537985e-02, 1.96179048e-02, 3.01269194e-02,\n",
              "        1.18208128e-02, 8.50487879e-02, 8.40085874e-03, 1.67146935e-02,\n",
              "        1.31957875e-02, 1.91421891e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrTpBeYFuIn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a721f3e-cd74-4d74-f8d9-b3ffb10120cb"
      },
      "source": [
        "km.cluster_centers_.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 94)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQg0CF11chKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 这里写了一些 function, 利用中心点的 94 个词里 tfidf 最大的 6 个词, 选出来了 \n",
        "# 上面的东西只能做出 cluster, 而背后的含义, 需要你自己去解读, 基于你自己的 domain knowledge 以及你对内容的理解\n",
        "# 也就是说, 聚类后的 analysis 是最重要的 \n",
        "\n",
        "print (\"<Document clustering result by K-means>\")\n",
        "\n",
        "# km.cluster_centers_ denotes the importances of each items in centroid.\n",
        "# We need to sort it in decreasing-order and get the top k items.\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
        "\n",
        "Cluster_keywords_summary = {}\n",
        "for i in range(num_clusters):\n",
        "    print (\"Cluster \" + str(i) + \" words:\", end='')\n",
        "    Cluster_keywords_summary[i] = []\n",
        "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
        "        Cluster_keywords_summary[i].append(tf_selected_words[ind])\n",
        "        print (tf_selected_words[ind] + \",\", end='')\n",
        "    print ()\n",
        "    \n",
        "    cluster_text = frame[frame.cluster==i].text.tolist()\n",
        "    print (\"Cluster \" + str(i) + \" text (\" + str(len(cluster_reviews)) + \" text): \")\n",
        "    print (\", \".join(cluster_text))\n",
        "    print ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYOZXL53E8VV",
        "colab_type": "text"
      },
      "source": [
        "# Part 5: Topic Modeling - Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPVbFNFE8VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use LDA for clustering\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components=3)\n",
        "\n",
        "# LDA 要求你必须用整数~ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FCACS4Vp9qFB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "31c8e46a-ec5a-42a9-8b77-81767b82a4a3"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# LDA requires integer values\n",
        "tfidf_model_lda = CountVectorizer(max_df=0.99, max_features=500,\n",
        "                                 min_df=0.01, stop_words='english',\n",
        "                                 tokenizer=tokenization_and_stemming, ngram_range=(1,1))\n",
        "\n",
        "tfidf_matrix_lda = tfidf_model_lda.fit_transform(data) #fit the vectorizer to synopses\n",
        "\n",
        "print (\"In total, there are \" + str(tfidf_matrix_lda.shape[0]) + \\\n",
        "      \" reviews and \" + str(tfidf_matrix_lda.shape[1]) + \" terms.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "In total, there are 7391 reviews and 94 terms.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PU80SW391II",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0f8b846d-a61c-4a5b-d52f-106f7e81cd84"
      },
      "source": [
        "# document topic matrix for tfidf_matrix_lda\n",
        "lda_output = lda.fit_transform(tfidf_matrix_lda)\n",
        "print(lda_output.shape)\n",
        "print(lda_output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7391, 3)\n",
            "[[0.05555895 0.75982874 0.18461231]\n",
            " [0.04177652 0.91515796 0.04306553]\n",
            " [0.11220793 0.77666032 0.11113175]\n",
            " ...\n",
            " [0.9046772  0.04763193 0.04769087]\n",
            " [0.91660329 0.04167638 0.04172033]\n",
            " [0.91660329 0.04167638 0.04172033]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab_type": "code",
        "id": "hCJs3lBz90vY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdb3a909-9ccf-424e-cd1e-77bc63d1e88b"
      },
      "source": [
        "# topics and words matrix\n",
        "topic_word = lda.components_\n",
        "print(topic_word.shape)\n",
        "print(topic_word)\n",
        "\n",
        "# (5,241) 每一个 topic 里面, 241 个词, 每一个词都有一个数对应, 这个数代表这个词儿的重要性"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 94)\n",
            "[[3.37074610e-01 3.35563699e-01 1.08133040e+03 3.34676640e-01\n",
            "  3.35848682e-01 3.36085306e-01 3.34663353e-01 7.21129206e+01\n",
            "  3.36266078e-01 3.36412586e-01 3.35037516e-01 3.39356239e-01\n",
            "  3.35760503e-01 3.35948638e-01 5.41330978e+02 5.42328022e+02\n",
            "  3.35308423e-01 1.55732515e+03 3.35400652e-01 3.36145377e-01\n",
            "  3.36291416e-01 3.35096965e-01 3.35462092e-01 3.35282942e-01\n",
            "  3.35765267e-01 3.36533259e-01 3.35474874e-01 3.35515273e-01\n",
            "  3.36692752e-01 3.35533536e-01 3.38582563e-01 1.07165648e+02\n",
            "  3.35723879e-01 9.65321652e+02 3.37832619e-01 3.37405938e-01\n",
            "  3.34911465e-01 3.36396107e-01 3.37302385e+02 3.35799663e-01\n",
            "  6.21299246e+02 3.36377368e-01 3.35601133e-01 3.36748693e-01\n",
            "  1.96113805e+02 3.38104649e-01 3.37569296e-01 3.37677251e-01\n",
            "  2.94152520e+00 1.75702293e+03 3.35480432e-01 3.35083782e-01\n",
            "  8.17321562e+02 3.36954529e-01 3.38383469e-01 3.34780296e-01\n",
            "  3.35395720e-01 3.35031763e-01 3.35675142e-01 3.34406887e-01\n",
            "  3.36016840e-01 3.40574367e-01 3.36076437e-01 3.35252446e-01\n",
            "  1.92832248e+03 5.41330709e+02 3.37117312e-01 8.17125802e+02\n",
            "  3.35998333e-01 3.35415218e-01 3.34213421e-01 3.35896491e-01\n",
            "  1.82832478e+03 3.36075281e-01 1.34263785e+02 3.34599806e-01\n",
            "  3.38759078e-01 3.42855978e-01 3.37639306e-01 3.35353581e-01\n",
            "  3.35688826e-01 3.35396076e-01 3.36591079e-01 5.59812491e+02\n",
            "  3.39244188e-01 3.36436758e-01 3.35525282e-01 3.35191756e-01\n",
            "  3.35821812e-01 3.34665706e-01 3.35097125e-01 3.35536907e-01\n",
            "  3.34535515e-01 3.35935878e-01]\n",
            " [5.53770255e+01 4.90036787e+01 3.34606552e-01 8.03101594e+01\n",
            "  3.60351372e-01 6.99375398e+01 3.54305412e-01 6.32616231e+01\n",
            "  6.26591511e+01 8.42404680e+01 2.15936889e+02 9.32807907e+01\n",
            "  7.58492685e+00 2.26250634e+01 3.34311882e-01 3.36028702e-01\n",
            "  1.31902716e+00 3.38176368e-01 3.66163560e-01 3.41141953e-01\n",
            "  7.32426145e+01 4.40543368e+00 4.60649789e+00 3.50852179e-01\n",
            "  1.10738341e+02 1.01252235e+01 3.89967680e-01 2.40001060e+02\n",
            "  5.51151784e+01 1.89996731e+02 3.57365545e+01 3.84975560e-01\n",
            "  9.25264123e+01 3.39588284e-01 1.56470354e+01 2.25680959e+01\n",
            "  2.28322622e+02 3.90653164e-01 3.59176413e-01 1.19636294e+00\n",
            "  3.49231440e-01 1.35359155e+01 1.58395201e+00 4.74686363e+01\n",
            "  6.29610615e+02 9.27914913e+00 4.00280392e+01 1.80802323e+02\n",
            "  1.02721117e+02 6.22178018e-01 9.54217580e+01 3.49983117e-01\n",
            "  2.61658507e+00 4.66062696e+01 3.45565247e-01 3.57316107e-01\n",
            "  1.62837379e+01 8.71967151e+01 1.56256339e+02 4.54737059e-01\n",
            "  1.30732367e+02 1.07748471e+01 1.06562399e+02 9.20771130e+01\n",
            "  3.39076553e-01 3.34738710e-01 5.51204253e+01 2.45581415e+00\n",
            "  3.68886089e-01 2.01183825e+01 8.92423212e+01 1.03062079e+02\n",
            "  3.37302707e-01 9.45298109e+01 3.82472541e-01 3.65818346e-01\n",
            "  3.51790643e+01 3.64527372e-01 8.44528094e+01 1.81867326e+02\n",
            "  1.65921254e+02 1.31208055e+02 4.34201264e+01 2.86702039e+02\n",
            "  3.11769908e+01 8.56663829e+01 1.22439873e+02 1.28186805e+02\n",
            "  8.23417894e+00 6.68321856e+02 5.25611165e+01 1.28930506e+02\n",
            "  1.05511332e+00 8.88166183e+01]\n",
            " [3.62858999e+01 9.85660758e+02 3.34998187e-01 3.55163934e-01\n",
            "  1.17303800e+02 4.72637488e+00 1.39311031e+02 2.16254563e+01\n",
            "  1.40045829e+01 4.23119422e-01 7.28073316e-01 3.79853023e-01\n",
            "  6.80793126e+01 1.23038988e+02 3.34710495e-01 3.35949574e-01\n",
            "  2.93345664e+02 3.36671279e-01 1.31298436e+02 9.73227127e+01\n",
            "  2.74210941e+01 8.22594694e+01 8.90580400e+01 7.93138649e+01\n",
            "  6.92589340e+00 7.45382433e+01 7.92745574e+01 6.63424628e-01\n",
            "  1.34548129e+02 6.67734998e-01 7.69248630e+01 7.64493768e+01\n",
            "  4.61378638e+01 3.38760094e-01 7.30151319e+01 5.60944981e+01\n",
            "  3.42466200e-01 1.67727295e+03 3.38438835e-01 8.04678374e+01\n",
            "  3.51522267e-01 1.73127707e+02 9.50804469e+01 3.81946150e+01\n",
            "  1.27558032e+00 7.83827462e+01 4.26343915e+01 1.58599996e+01\n",
            "  3.37357786e-01 3.54887533e-01 1.79242762e+02 8.03149331e+01\n",
            "  2.90618529e+01 5.10567759e+01 1.08316051e+02 1.14307904e+02\n",
            "  1.22380866e+02 1.46825314e+00 4.07986182e-01 2.84210856e+02\n",
            "  2.13931616e+02 7.38845785e+01 2.51015250e+01 5.87634584e-01\n",
            "  3.38444463e-01 3.34552386e-01 5.15424574e+01 2.41838383e+00\n",
            "  1.38295116e+02 9.85462022e+01 4.23465342e-01 1.60202494e+00\n",
            "  3.37912544e-01 2.01341138e+01 3.53742863e-01 9.42995818e+01\n",
            "  4.94821766e+01 1.29292617e+02 2.62095513e+01 7.97320432e-01\n",
            "  7.43057175e-01 1.39456549e+02 1.53243282e+02 4.85469398e-01\n",
            "  5.54837651e+01 8.49971804e+01 6.12246021e+01 2.11478004e+02\n",
            "  8.94299992e+01 3.43477859e-01 5.11037864e+01 7.33956898e-01\n",
            "  1.04610351e+02 1.04847446e+02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCEVfJS5AEgx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "2607dd53-7d88-486d-b43d-67e6d84e49d5"
      },
      "source": [
        "# column names\n",
        "topic_names = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
        "\n",
        "# index names\n",
        "doc_names = [\"Doc\" + str(i) for i in range(len(data))]\n",
        "\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)\n",
        "\n",
        "# get dominant topic for each document\n",
        "topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['topic'] = topic\n",
        "\n",
        "df_document_topic.head(10)\n",
        "\n",
        "# 取每个文档里 topic 值最大的那个 topic 最为这个文档的 topic \n",
        "# LDA 给了一个更加均匀的分布 \n",
        "# 下面是一个 1000 * 5 的矩阵, 每一个 topic 对应一个 probability, 取最大的, 做好聚类 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic0</th>\n",
              "      <th>Topic1</th>\n",
              "      <th>Topic2</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc0</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.18</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc1</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc2</th>\n",
              "      <td>0.11</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc3</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc4</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.89</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc5</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc6</th>\n",
              "      <td>0.26</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.38</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc7</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc8</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc9</th>\n",
              "      <td>0.11</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.77</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Topic0  Topic1  Topic2  topic\n",
              "Doc0    0.06    0.76    0.18      1\n",
              "Doc1    0.04    0.92    0.04      1\n",
              "Doc2    0.11    0.78    0.11      1\n",
              "Doc3    0.08    0.82    0.10      1\n",
              "Doc4    0.06    0.06    0.89      2\n",
              "Doc5    0.37    0.37    0.26      0\n",
              "Doc6    0.26    0.35    0.38      2\n",
              "Doc7    0.08    0.49    0.43      1\n",
              "Doc8    0.28    0.48    0.24      1\n",
              "Doc9    0.11    0.12    0.77      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InPLDW7kBSOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "3c6f9977-fa2b-4901-aeec-bb7dbe81f722"
      },
      "source": [
        "df_document_topic['topic'].value_counts().to_frame()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   topic\n",
              "0   2937\n",
              "2   2542\n",
              "1   1912"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yLe_RFHCz0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32ee658e-f36c-48be-d0ec-d8bf17425e2d"
      },
      "source": [
        "# topic word matrix\n",
        "print(lda.components_)\n",
        "# topic-word matrix\n",
        "df_topic_words = pd.DataFrame(lda.components_)\n",
        "\n",
        "# column and index\n",
        "df_topic_words.columns = tfidf_model_lda.get_feature_names()\n",
        "df_topic_words.index = topic_names\n",
        "\n",
        "df_topic_words.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.37074610e-01 3.35563699e-01 1.08133040e+03 3.34676640e-01\n",
            "  3.35848682e-01 3.36085306e-01 3.34663353e-01 7.21129206e+01\n",
            "  3.36266078e-01 3.36412586e-01 3.35037516e-01 3.39356239e-01\n",
            "  3.35760503e-01 3.35948638e-01 5.41330978e+02 5.42328022e+02\n",
            "  3.35308423e-01 1.55732515e+03 3.35400652e-01 3.36145377e-01\n",
            "  3.36291416e-01 3.35096965e-01 3.35462092e-01 3.35282942e-01\n",
            "  3.35765267e-01 3.36533259e-01 3.35474874e-01 3.35515273e-01\n",
            "  3.36692752e-01 3.35533536e-01 3.38582563e-01 1.07165648e+02\n",
            "  3.35723879e-01 9.65321652e+02 3.37832619e-01 3.37405938e-01\n",
            "  3.34911465e-01 3.36396107e-01 3.37302385e+02 3.35799663e-01\n",
            "  6.21299246e+02 3.36377368e-01 3.35601133e-01 3.36748693e-01\n",
            "  1.96113805e+02 3.38104649e-01 3.37569296e-01 3.37677251e-01\n",
            "  2.94152520e+00 1.75702293e+03 3.35480432e-01 3.35083782e-01\n",
            "  8.17321562e+02 3.36954529e-01 3.38383469e-01 3.34780296e-01\n",
            "  3.35395720e-01 3.35031763e-01 3.35675142e-01 3.34406887e-01\n",
            "  3.36016840e-01 3.40574367e-01 3.36076437e-01 3.35252446e-01\n",
            "  1.92832248e+03 5.41330709e+02 3.37117312e-01 8.17125802e+02\n",
            "  3.35998333e-01 3.35415218e-01 3.34213421e-01 3.35896491e-01\n",
            "  1.82832478e+03 3.36075281e-01 1.34263785e+02 3.34599806e-01\n",
            "  3.38759078e-01 3.42855978e-01 3.37639306e-01 3.35353581e-01\n",
            "  3.35688826e-01 3.35396076e-01 3.36591079e-01 5.59812491e+02\n",
            "  3.39244188e-01 3.36436758e-01 3.35525282e-01 3.35191756e-01\n",
            "  3.35821812e-01 3.34665706e-01 3.35097125e-01 3.35536907e-01\n",
            "  3.34535515e-01 3.35935878e-01]\n",
            " [5.53770255e+01 4.90036787e+01 3.34606552e-01 8.03101594e+01\n",
            "  3.60351372e-01 6.99375398e+01 3.54305412e-01 6.32616231e+01\n",
            "  6.26591511e+01 8.42404680e+01 2.15936889e+02 9.32807907e+01\n",
            "  7.58492685e+00 2.26250634e+01 3.34311882e-01 3.36028702e-01\n",
            "  1.31902716e+00 3.38176368e-01 3.66163560e-01 3.41141953e-01\n",
            "  7.32426145e+01 4.40543368e+00 4.60649789e+00 3.50852179e-01\n",
            "  1.10738341e+02 1.01252235e+01 3.89967680e-01 2.40001060e+02\n",
            "  5.51151784e+01 1.89996731e+02 3.57365545e+01 3.84975560e-01\n",
            "  9.25264123e+01 3.39588284e-01 1.56470354e+01 2.25680959e+01\n",
            "  2.28322622e+02 3.90653164e-01 3.59176413e-01 1.19636294e+00\n",
            "  3.49231440e-01 1.35359155e+01 1.58395201e+00 4.74686363e+01\n",
            "  6.29610615e+02 9.27914913e+00 4.00280392e+01 1.80802323e+02\n",
            "  1.02721117e+02 6.22178018e-01 9.54217580e+01 3.49983117e-01\n",
            "  2.61658507e+00 4.66062696e+01 3.45565247e-01 3.57316107e-01\n",
            "  1.62837379e+01 8.71967151e+01 1.56256339e+02 4.54737059e-01\n",
            "  1.30732367e+02 1.07748471e+01 1.06562399e+02 9.20771130e+01\n",
            "  3.39076553e-01 3.34738710e-01 5.51204253e+01 2.45581415e+00\n",
            "  3.68886089e-01 2.01183825e+01 8.92423212e+01 1.03062079e+02\n",
            "  3.37302707e-01 9.45298109e+01 3.82472541e-01 3.65818346e-01\n",
            "  3.51790643e+01 3.64527372e-01 8.44528094e+01 1.81867326e+02\n",
            "  1.65921254e+02 1.31208055e+02 4.34201264e+01 2.86702039e+02\n",
            "  3.11769908e+01 8.56663829e+01 1.22439873e+02 1.28186805e+02\n",
            "  8.23417894e+00 6.68321856e+02 5.25611165e+01 1.28930506e+02\n",
            "  1.05511332e+00 8.88166183e+01]\n",
            " [3.62858999e+01 9.85660758e+02 3.34998187e-01 3.55163934e-01\n",
            "  1.17303800e+02 4.72637488e+00 1.39311031e+02 2.16254563e+01\n",
            "  1.40045829e+01 4.23119422e-01 7.28073316e-01 3.79853023e-01\n",
            "  6.80793126e+01 1.23038988e+02 3.34710495e-01 3.35949574e-01\n",
            "  2.93345664e+02 3.36671279e-01 1.31298436e+02 9.73227127e+01\n",
            "  2.74210941e+01 8.22594694e+01 8.90580400e+01 7.93138649e+01\n",
            "  6.92589340e+00 7.45382433e+01 7.92745574e+01 6.63424628e-01\n",
            "  1.34548129e+02 6.67734998e-01 7.69248630e+01 7.64493768e+01\n",
            "  4.61378638e+01 3.38760094e-01 7.30151319e+01 5.60944981e+01\n",
            "  3.42466200e-01 1.67727295e+03 3.38438835e-01 8.04678374e+01\n",
            "  3.51522267e-01 1.73127707e+02 9.50804469e+01 3.81946150e+01\n",
            "  1.27558032e+00 7.83827462e+01 4.26343915e+01 1.58599996e+01\n",
            "  3.37357786e-01 3.54887533e-01 1.79242762e+02 8.03149331e+01\n",
            "  2.90618529e+01 5.10567759e+01 1.08316051e+02 1.14307904e+02\n",
            "  1.22380866e+02 1.46825314e+00 4.07986182e-01 2.84210856e+02\n",
            "  2.13931616e+02 7.38845785e+01 2.51015250e+01 5.87634584e-01\n",
            "  3.38444463e-01 3.34552386e-01 5.15424574e+01 2.41838383e+00\n",
            "  1.38295116e+02 9.85462022e+01 4.23465342e-01 1.60202494e+00\n",
            "  3.37912544e-01 2.01341138e+01 3.53742863e-01 9.42995818e+01\n",
            "  4.94821766e+01 1.29292617e+02 2.62095513e+01 7.97320432e-01\n",
            "  7.43057175e-01 1.39456549e+02 1.53243282e+02 4.85469398e-01\n",
            "  5.54837651e+01 8.49971804e+01 6.12246021e+01 2.11478004e+02\n",
            "  8.94299992e+01 3.43477859e-01 5.11037864e+01 7.33956898e-01\n",
            "  1.04610351e+02 1.04847446e+02]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>'d</th>\n",
              "      <th>'s</th>\n",
              "      <th>account</th>\n",
              "      <th>alway</th>\n",
              "      <th>amp</th>\n",
              "      <th>away</th>\n",
              "      <th>best</th>\n",
              "      <th>better</th>\n",
              "      <th>big</th>\n",
              "      <th>ca</th>\n",
              "      <th>check</th>\n",
              "      <th>chicken</th>\n",
              "      <th>click</th>\n",
              "      <th>come</th>\n",
              "      <th>contact</th>\n",
              "      <th>countri</th>\n",
              "      <th>day</th>\n",
              "      <th>dm</th>\n",
              "      <th>dog</th>\n",
              "      <th>e-mail</th>\n",
              "      <th>enter</th>\n",
              "      <th>episod</th>\n",
              "      <th>everi</th>\n",
              "      <th>favorit</th>\n",
              "      <th>feel</th>\n",
              "      <th>film</th>\n",
              "      <th>friend</th>\n",
              "      <th>good</th>\n",
              "      <th>got</th>\n",
              "      <th>great</th>\n",
              "      <th>guy</th>\n",
              "      <th>happen</th>\n",
              "      <th>happi</th>\n",
              "      <th>hear</th>\n",
              "      <th>help</th>\n",
              "      <th>hope</th>\n",
              "      <th>http</th>\n",
              "      <th>https</th>\n",
              "      <th>inform</th>\n",
              "      <th>itûª</th>\n",
              "      <th>...</th>\n",
              "      <th>moosejaw.com</th>\n",
              "      <th>moosejawmad</th>\n",
              "      <th>movi</th>\n",
              "      <th>na</th>\n",
              "      <th>need</th>\n",
              "      <th>netflix</th>\n",
              "      <th>new</th>\n",
              "      <th>order</th>\n",
              "      <th>peopl</th>\n",
              "      <th>perfect</th>\n",
              "      <th>pleas</th>\n",
              "      <th>reach</th>\n",
              "      <th>realli</th>\n",
              "      <th>right</th>\n",
              "      <th>say</th>\n",
              "      <th>season</th>\n",
              "      <th>seen</th>\n",
              "      <th>someth</th>\n",
              "      <th>sorri</th>\n",
              "      <th>start</th>\n",
              "      <th>store</th>\n",
              "      <th>stori</th>\n",
              "      <th>sure</th>\n",
              "      <th>tell</th>\n",
              "      <th>thank</th>\n",
              "      <th>thing</th>\n",
              "      <th>think</th>\n",
              "      <th>time</th>\n",
              "      <th>today</th>\n",
              "      <th>tri</th>\n",
              "      <th>use</th>\n",
              "      <th>ve</th>\n",
              "      <th>want</th>\n",
              "      <th>watch</th>\n",
              "      <th>way</th>\n",
              "      <th>whattowatchonnetflix</th>\n",
              "      <th>win</th>\n",
              "      <th>work</th>\n",
              "      <th>world</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic0</th>\n",
              "      <td>0.337075</td>\n",
              "      <td>0.335564</td>\n",
              "      <td>1081.330395</td>\n",
              "      <td>0.334677</td>\n",
              "      <td>0.335849</td>\n",
              "      <td>0.336085</td>\n",
              "      <td>0.334663</td>\n",
              "      <td>72.112921</td>\n",
              "      <td>0.336266</td>\n",
              "      <td>0.336413</td>\n",
              "      <td>0.335038</td>\n",
              "      <td>0.339356</td>\n",
              "      <td>0.335761</td>\n",
              "      <td>0.335949</td>\n",
              "      <td>541.330978</td>\n",
              "      <td>542.328022</td>\n",
              "      <td>0.335308</td>\n",
              "      <td>1557.325152</td>\n",
              "      <td>0.335401</td>\n",
              "      <td>0.336145</td>\n",
              "      <td>0.336291</td>\n",
              "      <td>0.335097</td>\n",
              "      <td>0.335462</td>\n",
              "      <td>0.335283</td>\n",
              "      <td>0.335765</td>\n",
              "      <td>0.336533</td>\n",
              "      <td>0.335475</td>\n",
              "      <td>0.335515</td>\n",
              "      <td>0.336693</td>\n",
              "      <td>0.335534</td>\n",
              "      <td>0.338583</td>\n",
              "      <td>107.165648</td>\n",
              "      <td>0.335724</td>\n",
              "      <td>965.321652</td>\n",
              "      <td>0.337833</td>\n",
              "      <td>0.337406</td>\n",
              "      <td>0.334911</td>\n",
              "      <td>0.336396</td>\n",
              "      <td>337.302385</td>\n",
              "      <td>0.335800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.338383</td>\n",
              "      <td>0.334780</td>\n",
              "      <td>0.335396</td>\n",
              "      <td>0.335032</td>\n",
              "      <td>0.335675</td>\n",
              "      <td>0.334407</td>\n",
              "      <td>0.336017</td>\n",
              "      <td>0.340574</td>\n",
              "      <td>0.336076</td>\n",
              "      <td>0.335252</td>\n",
              "      <td>1928.322479</td>\n",
              "      <td>541.330709</td>\n",
              "      <td>0.337117</td>\n",
              "      <td>817.125802</td>\n",
              "      <td>0.335998</td>\n",
              "      <td>0.335415</td>\n",
              "      <td>0.334213</td>\n",
              "      <td>0.335896</td>\n",
              "      <td>1828.324785</td>\n",
              "      <td>0.336075</td>\n",
              "      <td>134.263785</td>\n",
              "      <td>0.334600</td>\n",
              "      <td>0.338759</td>\n",
              "      <td>0.342856</td>\n",
              "      <td>0.337639</td>\n",
              "      <td>0.335354</td>\n",
              "      <td>0.335689</td>\n",
              "      <td>0.335396</td>\n",
              "      <td>0.336591</td>\n",
              "      <td>559.812491</td>\n",
              "      <td>0.339244</td>\n",
              "      <td>0.336437</td>\n",
              "      <td>0.335525</td>\n",
              "      <td>0.335192</td>\n",
              "      <td>0.335822</td>\n",
              "      <td>0.334666</td>\n",
              "      <td>0.335097</td>\n",
              "      <td>0.335537</td>\n",
              "      <td>0.334536</td>\n",
              "      <td>0.335936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic1</th>\n",
              "      <td>55.377025</td>\n",
              "      <td>49.003679</td>\n",
              "      <td>0.334607</td>\n",
              "      <td>80.310159</td>\n",
              "      <td>0.360351</td>\n",
              "      <td>69.937540</td>\n",
              "      <td>0.354305</td>\n",
              "      <td>63.261623</td>\n",
              "      <td>62.659151</td>\n",
              "      <td>84.240468</td>\n",
              "      <td>215.936889</td>\n",
              "      <td>93.280791</td>\n",
              "      <td>7.584927</td>\n",
              "      <td>22.625063</td>\n",
              "      <td>0.334312</td>\n",
              "      <td>0.336029</td>\n",
              "      <td>1.319027</td>\n",
              "      <td>0.338176</td>\n",
              "      <td>0.366164</td>\n",
              "      <td>0.341142</td>\n",
              "      <td>73.242615</td>\n",
              "      <td>4.405434</td>\n",
              "      <td>4.606498</td>\n",
              "      <td>0.350852</td>\n",
              "      <td>110.738341</td>\n",
              "      <td>10.125223</td>\n",
              "      <td>0.389968</td>\n",
              "      <td>240.001060</td>\n",
              "      <td>55.115178</td>\n",
              "      <td>189.996731</td>\n",
              "      <td>35.736554</td>\n",
              "      <td>0.384976</td>\n",
              "      <td>92.526412</td>\n",
              "      <td>0.339588</td>\n",
              "      <td>15.647035</td>\n",
              "      <td>22.568096</td>\n",
              "      <td>228.322622</td>\n",
              "      <td>0.390653</td>\n",
              "      <td>0.359176</td>\n",
              "      <td>1.196363</td>\n",
              "      <td>...</td>\n",
              "      <td>0.345565</td>\n",
              "      <td>0.357316</td>\n",
              "      <td>16.283738</td>\n",
              "      <td>87.196715</td>\n",
              "      <td>156.256339</td>\n",
              "      <td>0.454737</td>\n",
              "      <td>130.732367</td>\n",
              "      <td>10.774847</td>\n",
              "      <td>106.562399</td>\n",
              "      <td>92.077113</td>\n",
              "      <td>0.339077</td>\n",
              "      <td>0.334739</td>\n",
              "      <td>55.120425</td>\n",
              "      <td>2.455814</td>\n",
              "      <td>0.368886</td>\n",
              "      <td>20.118383</td>\n",
              "      <td>89.242321</td>\n",
              "      <td>103.062079</td>\n",
              "      <td>0.337303</td>\n",
              "      <td>94.529811</td>\n",
              "      <td>0.382473</td>\n",
              "      <td>0.365818</td>\n",
              "      <td>35.179064</td>\n",
              "      <td>0.364527</td>\n",
              "      <td>84.452809</td>\n",
              "      <td>181.867326</td>\n",
              "      <td>165.921254</td>\n",
              "      <td>131.208055</td>\n",
              "      <td>43.420126</td>\n",
              "      <td>286.702039</td>\n",
              "      <td>31.176991</td>\n",
              "      <td>85.666383</td>\n",
              "      <td>122.439873</td>\n",
              "      <td>128.186805</td>\n",
              "      <td>8.234179</td>\n",
              "      <td>668.321856</td>\n",
              "      <td>52.561117</td>\n",
              "      <td>128.930506</td>\n",
              "      <td>1.055113</td>\n",
              "      <td>88.816618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic2</th>\n",
              "      <td>36.285900</td>\n",
              "      <td>985.660758</td>\n",
              "      <td>0.334998</td>\n",
              "      <td>0.355164</td>\n",
              "      <td>117.303800</td>\n",
              "      <td>4.726375</td>\n",
              "      <td>139.311031</td>\n",
              "      <td>21.625456</td>\n",
              "      <td>14.004583</td>\n",
              "      <td>0.423119</td>\n",
              "      <td>0.728073</td>\n",
              "      <td>0.379853</td>\n",
              "      <td>68.079313</td>\n",
              "      <td>123.038988</td>\n",
              "      <td>0.334710</td>\n",
              "      <td>0.335950</td>\n",
              "      <td>293.345664</td>\n",
              "      <td>0.336671</td>\n",
              "      <td>131.298436</td>\n",
              "      <td>97.322713</td>\n",
              "      <td>27.421094</td>\n",
              "      <td>82.259469</td>\n",
              "      <td>89.058040</td>\n",
              "      <td>79.313865</td>\n",
              "      <td>6.925893</td>\n",
              "      <td>74.538243</td>\n",
              "      <td>79.274557</td>\n",
              "      <td>0.663425</td>\n",
              "      <td>134.548129</td>\n",
              "      <td>0.667735</td>\n",
              "      <td>76.924863</td>\n",
              "      <td>76.449377</td>\n",
              "      <td>46.137864</td>\n",
              "      <td>0.338760</td>\n",
              "      <td>73.015132</td>\n",
              "      <td>56.094498</td>\n",
              "      <td>0.342466</td>\n",
              "      <td>1677.272951</td>\n",
              "      <td>0.338439</td>\n",
              "      <td>80.467837</td>\n",
              "      <td>...</td>\n",
              "      <td>108.316051</td>\n",
              "      <td>114.307904</td>\n",
              "      <td>122.380866</td>\n",
              "      <td>1.468253</td>\n",
              "      <td>0.407986</td>\n",
              "      <td>284.210856</td>\n",
              "      <td>213.931616</td>\n",
              "      <td>73.884579</td>\n",
              "      <td>25.101525</td>\n",
              "      <td>0.587635</td>\n",
              "      <td>0.338444</td>\n",
              "      <td>0.334552</td>\n",
              "      <td>51.542457</td>\n",
              "      <td>2.418384</td>\n",
              "      <td>138.295116</td>\n",
              "      <td>98.546202</td>\n",
              "      <td>0.423465</td>\n",
              "      <td>1.602025</td>\n",
              "      <td>0.337913</td>\n",
              "      <td>20.134114</td>\n",
              "      <td>0.353743</td>\n",
              "      <td>94.299582</td>\n",
              "      <td>49.482177</td>\n",
              "      <td>129.292617</td>\n",
              "      <td>26.209551</td>\n",
              "      <td>0.797320</td>\n",
              "      <td>0.743057</td>\n",
              "      <td>139.456549</td>\n",
              "      <td>153.243282</td>\n",
              "      <td>0.485469</td>\n",
              "      <td>55.483765</td>\n",
              "      <td>84.997180</td>\n",
              "      <td>61.224602</td>\n",
              "      <td>211.478004</td>\n",
              "      <td>89.429999</td>\n",
              "      <td>0.343478</td>\n",
              "      <td>51.103786</td>\n",
              "      <td>0.733957</td>\n",
              "      <td>104.610351</td>\n",
              "      <td>104.847446</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 94 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               'd          's      account  ...        work       world        year\n",
              "Topic0   0.337075    0.335564  1081.330395  ...    0.335537    0.334536    0.335936\n",
              "Topic1  55.377025   49.003679     0.334607  ...  128.930506    1.055113   88.816618\n",
              "Topic2  36.285900  985.660758     0.334998  ...    0.733957  104.610351  104.847446\n",
              "\n",
              "[3 rows x 94 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbU9U8V-DFDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "f3b34193-e9bd-4062-9e7f-3be0ee8ef6d6"
      },
      "source": [
        "# print top n keywords for each topic\n",
        "def print_topic_words(tfidf_model, lda_model, n_words):\n",
        "    words = np.array(tfidf_model.get_feature_names())\n",
        "    topic_words = []\n",
        "    # for each topic, we have words weight\n",
        "    for topic_words_weights in lda_model.components_:\n",
        "        top_words = topic_words_weights.argsort()[::-1][:n_words]\n",
        "        topic_words.append(words.take(top_words))\n",
        "    return topic_words\n",
        "\n",
        "topic_keywords = print_topic_words(tfidf_model=tfidf_model_lda, lda_model=lda, n_words=15)        \n",
        "\n",
        "df_topic_words = pd.DataFrame(topic_keywords)\n",
        "df_topic_words.columns = ['Word '+str(i) for i in range(df_topic_words.shape[1])]\n",
        "df_topic_words.index = ['Topic '+str(i) for i in range(df_topic_words.shape[0])]\n",
        "df_topic_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word 0</th>\n",
              "      <th>Word 1</th>\n",
              "      <th>Word 2</th>\n",
              "      <th>Word 3</th>\n",
              "      <th>Word 4</th>\n",
              "      <th>Word 5</th>\n",
              "      <th>Word 6</th>\n",
              "      <th>Word 7</th>\n",
              "      <th>Word 8</th>\n",
              "      <th>Word 9</th>\n",
              "      <th>Word 10</th>\n",
              "      <th>Word 11</th>\n",
              "      <th>Word 12</th>\n",
              "      <th>Word 13</th>\n",
              "      <th>Word 14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Topic 0</th>\n",
              "      <td>pleas</td>\n",
              "      <td>sorri</td>\n",
              "      <td>look</td>\n",
              "      <td>dm</td>\n",
              "      <td>account</td>\n",
              "      <td>hear</td>\n",
              "      <td>make</td>\n",
              "      <td>right</td>\n",
              "      <td>kfc</td>\n",
              "      <td>tri</td>\n",
              "      <td>countri</td>\n",
              "      <td>contact</td>\n",
              "      <td>reach</td>\n",
              "      <td>inform</td>\n",
              "      <td>like</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 1</th>\n",
              "      <td>whattowatchonnetflix</td>\n",
              "      <td>like</td>\n",
              "      <td>tri</td>\n",
              "      <td>good</td>\n",
              "      <td>http</td>\n",
              "      <td>check</td>\n",
              "      <td>great</td>\n",
              "      <td>thing</td>\n",
              "      <td>ll</td>\n",
              "      <td>think</td>\n",
              "      <td>need</td>\n",
              "      <td>time</td>\n",
              "      <td>new</td>\n",
              "      <td>work</td>\n",
              "      <td>watch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Topic 2</th>\n",
              "      <td>https</td>\n",
              "      <td>'s</td>\n",
              "      <td>day</td>\n",
              "      <td>netflix</td>\n",
              "      <td>new</td>\n",
              "      <td>watch</td>\n",
              "      <td>love</td>\n",
              "      <td>know</td>\n",
              "      <td>today</td>\n",
              "      <td>time</td>\n",
              "      <td>best</td>\n",
              "      <td>say</td>\n",
              "      <td>got</td>\n",
              "      <td>dog</td>\n",
              "      <td>tell</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Word 0 Word 1 Word 2  ... Word 12 Word 13 Word 14\n",
              "Topic 0                 pleas  sorri   look  ...   reach  inform    like\n",
              "Topic 1  whattowatchonnetflix   like    tri  ...     new    work   watch\n",
              "Topic 2                 https     's    day  ...     got     dog    tell\n",
              "\n",
              "[3 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}